# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TKJXz0-o_TuUp9p45eJbTp6-KVcwYmav
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import ExponentialLR

def function(x):
    return 1.0 / (1.0 + 25.0 * x**2)

def derivative(x):
    return -50.0 * x / ((1.0 + 25.0 * x**2)**2)

class NeuralNet(nn.Module):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1, 64), nn.Tanh(),
            nn.Linear(64, 64), nn.Tanh(),
            nn.Linear(64, 1)
        )
    def forward(self, x):
        return self.network(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} for training")

model = NeuralNet().to(device)

learning_rate = 0.0001
num_epochs = 20000
num_train_points = 200
num_val_points = 100
lambda_derivative = 0.1

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
scheduler = ExponentialLR(optimizer, gamma=0.9995)
mse_loss = nn.MSELoss()

x_train_np = np.random.uniform(-1, 1, num_train_points).reshape(-1, 1)
y_train_np = function(x_train_np)
y_train_derivative_np = derivative(x_train_np)

x_train = torch.tensor(x_train_np, dtype=torch.float32, requires_grad=True).to(device)
y_train = torch.tensor(y_train_np, dtype=torch.float32).to(device)
y_train_derivative = torch.tensor(y_train_derivative_np, dtype=torch.float32).to(device)

x_val_np = np.linspace(-1, 1, num_val_points).reshape(-1, 1)
y_val_np = function(x_val_np)
y_val_derivative_np = derivative(x_val_np)

x_val = torch.tensor(x_val_np, dtype=torch.float32, requires_grad=True).to(device)
y_val = torch.tensor(y_val_np, dtype=torch.float32).to(device)
y_val_derivative = torch.tensor(y_val_derivative_np, dtype=torch.float32).to(device)

train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()

    y_pred = model(x_train)
    y_pred_derivative = torch.autograd.grad(
        y_pred, x_train, grad_outputs=torch.ones_like(y_pred), create_graph=True
    )[0]

    train_func_loss = mse_loss(y_pred, y_train)
    train_deriv_loss = mse_loss(y_pred_derivative, y_train_derivative)
    total_train_loss = train_func_loss + lambda_derivative * train_deriv_loss

    optimizer.zero_grad()
    total_train_loss.backward()
    optimizer.step()
    scheduler.step()

    if (epoch + 1) % 10 == 0:
        model.eval()

        y_val_pred = model(x_val)
        y_val_pred_derivative = torch.autograd.grad(
            y_val_pred, x_val, grad_outputs=torch.ones_like(y_val_pred), retain_graph=True
        )[0]

        with torch.no_grad():
            val_func_loss = mse_loss(y_val_pred, y_val)
            val_deriv_loss = mse_loss(y_val_pred_derivative, y_val_derivative)
            total_val_loss = val_func_loss + lambda_derivative * val_deriv_loss

        train_losses.append(total_train_loss.item())
        val_losses.append(total_val_loss.item())

        if (epoch + 1) % 2000 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_train_loss.item():.8f}, Val Loss: {total_val_loss.item():.8f}')

model.eval()
x_test_np = np.linspace(-1, 1, 500).reshape(-1, 1)
x_test = torch.tensor(x_test_np, dtype=torch.float32, requires_grad=True).to(device)

y_true_f = runge_function(x_test_np)
y_true_d = runge_derivative(x_test_np)

y_pred_f = model(x_test)
y_pred_d = torch.autograd.grad(y_pred_f, x_test, grad_outputs=torch.ones_like(y_pred_f))[0]

y_pred_f_np = y_pred_f.cpu().detach().numpy()
y_pred_d_np = y_pred_d.cpu().detach().numpy()

mse_f = np.mean((y_true_f - y_pred_f_np)**2)
mse_d = np.mean((y_true_d - y_pred_d_np)**2)
max_err_f = np.max(np.abs(y_true_f - y_pred_f_np))
max_err_d = np.max(np.abs(y_true_d - y_pred_d_np))

print(f"Function Approximation MSE      : {mse_f:.8f}")
print(f"Function Approximation Max Error: {max_err_f:.8f}")
print(f"Derivative Approximation MSE    : {mse_d:.8f}")
print(f"Derivative Approximation Max Error: {max_err_d:.8f}")

plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
plt.plot(x_test_np, y_true_f, 'b-', label='True Function $f(x)$', linewidth=2)
plt.plot(x_test_np, y_pred_f_np, 'r--', label=r'NN Prediction $\hat{f}(x)$', linewidth=2)
plt.title('Function Approximation')
plt.xlabel('x'); plt.ylabel('f(x)'); plt.legend(); plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(x_test_np, y_true_d, 'b-', label="True Derivative $f'(x)$", linewidth=2)
plt.plot(x_test_np, y_pred_d_np, 'r--', label=r"NN Prediction $\hat{f}'(x)$", linewidth=2)
plt.title('Function Derivative Approximation')
plt.xlabel('x'); plt.ylabel("f'(x)"); plt.legend(); plt.grid(True)

plt.subplot(1, 3, 3)
loss_epochs = np.arange(len(train_losses)) * 10
plt.plot(loss_epochs, train_losses, 'g-', label='Training Loss')
plt.plot(loss_epochs, val_losses, 'orange', linestyle='--', label='Validation Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch'); plt.ylabel('Total Loss'); plt.yscale('log'); plt.legend(); plt.grid(True)

plt.tight_layout()
plt.show()