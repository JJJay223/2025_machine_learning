# Written assignment
## 1.Explain the concept of score matching and describe how it is used in score-based (diffusion) generative models.
在生成式模型中，核心目標是要學習真實數據的機率密度函數 p(x)，但這非常困難。因此有一種方法是轉而學習Score Function，其定義為數據分佈之對數機率密度 (log-probability density) 

對於輸入數據 x 的梯度 (gradient)：$S(x) = \nabla_x \log p(x)$

Score Matching 的目標是訓練一個神經網路模型 $S(x; θ)$，使其能夠準確估計出真實數據分佈的分數函數。

最直觀的訓練目標是最小化模型預測分數與真實分數之間的差距，稱為Explicit Score Matching(ESM)。但ESM無法得知真實的 $ \nabla_x \log p(x)$是什麼。

為了解決這個問題，因此提出了(Implicit Score Matching, ISM)，透過數學推導，將目標函數轉換成一個不需要知道真實分數的等價形式，從而讓模型可以被訓練。

Denoising Score Matching, DSM是Score Matching中最重要且最實用的一種變體，也是擴散模型的基礎，訓練一個模型，讓它學會如何從一張「有雜訊的圖片」中，估計出當初被加入的「雜訊」是什麼。

透過對數據加噪：不直接學習原始乾淨數據的分數，而是先對原始數據 $x_0$ 加入一些雜訊（ex.高斯雜訊 $ε$），得到一個雜訊數據 x。

學習雜訊數據的分數：模型的目標是學習這個「加噪後」的數據分佈 p(x) 的分數函數。

簡化的目標：透過數學推導可以證明，學習加噪後數據的分數，等價於訓練模型去預測從乾淨數據 $x_0$ 變成雜訊數據 x 時所加入的「雜訊 ε」本身。使得模型訓練變得可行且穩定。

基於分數的生成式模型（或稱擴散模型，如 DDPM）利用學習到的分數函數來生成新的數據。這個過程包含兩個階段：

1. 前向過程 (Forward Process / Diffusion Process)
這個過程是固定的，不需要學習。它從一張真實的、乾淨的數據 $x_0$ 開始，在數百到數千個時間步 (steps) 中，逐步地、微量地對數據加入高斯雜訊。經過足夠多的步驟後，原始數據會完全變成一個純粹的高斯雜訊分佈，原始的數據特徵不復存在。

2. 逆向過程 (Reverse Process / Generation Process)
從雜訊開始：從一個完全隨機的高斯雜訊 $x_t$ 開始。
逐步去噪：在每一步 t，我們使用透過「去噪分數匹配」(DSM) 訓練好的分數模型 $S(x_t; θ)$ 來估計當前雜訊數據 $x_t$ 的分數（梯度）。

梯度引導：這個分數 $ \nabla_x \log p(x_t) $ 表示數據密度增加的方向。模型會朝著這個方向移動一小步，使 $x_t$ 變得「稍微不那麼像雜訊」，而「稍微更像真實數據」。可看作是Langevin Dynamics的一種應用。

迭代生成：重複這個去噪步驟，從 $x_t$ 一路反推回 $x_{t-1}$, $x_{t-2}$, ..., 直到 $x_0$。最終得到的 $x_0$ 就是一個全新的、由模型生成的、看起來非常逼真的數據樣本。

總結來說，分數匹配 (特別是去噪分數匹配) 負責訓練一個能夠估計雜訊方向的模型。

而擴散模型則利用這個訓練好的模型，從一個純粹的隨機雜訊開始，一步步地將雜訊去除，最終還原出一張清晰、高品質的生成圖像。

## 2. Unanswered Questions

在 DSM 中，加入的雜訊強度 σ 是否會影響模型的學習效果？如果雜訊太小或太大，會有啥效果?